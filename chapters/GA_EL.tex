\cheatbox{1. Genetic Algorithm (GA)}{\textStrech}{
\footnotesize
\textbf{1. Khái niệm cốt lõi:}
\begin{itemize}[leftmargin=*, nosep]
    \item \textbf{Học là tìm kiếm:} Tìm giả thuyết tốt nhất trong không gian giả thuyết thông qua các thế hệ.
    \item Thế hệ giả thuyết tiếp theo được tạo ra từ đột biến và lai ghép các giả thuyết tốt của thế hệ hiện tại.
\end{itemize}
}

\cheatbox{}{\textStrech}{
\footnotesize
\textbf{2. Ưu điểm:}
\begin{itemize}[leftmargin=*, nosep]
    \item Phương pháp bền vững (robust) để thích nghi.
    \item Xử lý được không gian giả thuyết chứa các phần tương tác phức tạp.
    \item Dễ dàng song song hóa (Parallelized).
\end{itemize}

\textbf{3. Giải thuật GA tiêu chuẩn:}
\begin{itemize}[leftmargin=*, nosep]
    \item \textbf{B1. Khởi tạo:} Quần thể $P$ gồm $p$ giả thuyết ngẫu nhiên.
    \item \textbf{B2. Đánh giá:} Tính $Fitness(h)$ cho mỗi $h \in P$.
    \item \textbf{B3. While $\max_{h \in P} Fitness(h) < Fitness\_threshold$ do:}
    \begin{itemize}[nosep]
        \item Tạo thế hệ mới (New Generation).
        \item Đánh giá lại $Fitness$.
    \end{itemize}
\end{itemize}
}

\cheatbox{2. Tạo thế hệ mới}{\textStrech}{
\footnotesize
\textbf{1. Chọn lọc (Selection):}
\begin{itemize}[leftmargin=*, nosep]
    \item Chọn $(1-r)p$ giả thuyết từ $P$ để thêm vào thế hệ mới.
    \item \textbf{Xác suất chọn (Probabilistic Selection):}
    $$ Pr(h_i) = \frac{Fitness(h_i)}{\sum_{h \in P} Fitness(h)} $$
    (Giả thuyết tốt hơn có cơ hội được chọn cao hơn).
\end{itemize}

\textbf{2. Lai ghép (Crossover):}
\begin{itemize}[leftmargin=*, nosep]
    \item Chọn $(r/2)p$ cặp giả thuyết từ $P$ dựa theo $Pr(h)$.
    \item Với mỗi cặp $(h_1, h_2)$, áp dụng toán tử lai ghép để tạo 2 cá thể con. Sau đó thêm tất cả con vào thế hệ mới.
\end{itemize}

\textbf{3. Đột biến (Mutation):} Chọn ngẫu nhiên $m\%$ các giả thuyết vừa thêm vào với phân phối đều. Với mỗi giả thuyết, đảo ngược 1 bit ngẫu nhiên trong biểu diễn của nó.
}

\cheatbox{3. Biểu diễn Giả thuyết}{\textStrech}{
\footnotesize
\textbf{1. Biểu diễn Luật phân loại (Classification Rule):}
\begin{itemize}[leftmargin=*, nosep]
    \item Dạng bit string: IF expr(A1)$\land$...$\land$expr(An) THEN C = c.
    \item Mã hóa thành chuỗi bit. Độ dài chuỗi bit phụ thuộc vào số lượng thuộc tính và số giá trị có thể có của chúng.
\end{itemize}

\textbf{2. Ví dụ Mã hóa:} Thuộc tính \underline{Wind} (Strong, Weak) = 2 bit (Strong=10; Weak=01; Không quan tâm=11). Chuỗi đầy đủ ghép các thuộc tính: (Outlook | Wind | PlayTennis) là 3 bit | 2 bit | 2 bit = 111|10|10 = 11110110.

\textbf{3. Hàm Fitness:}
$Fitness(h) = (correct(h))^2$. Với $correct(h)$ là phần trăm số điểm dữ liệu phân loại đúng bởi giả thuyết $h$.
}

\cheatbox{4. Các Toán tử Lai ghép Chi tiết}{\textStrech}{
\footnotesize
\textbf{1. Lai ghép một điểm:} Chọn 1 điểm cắt ngẫu nhiên trên chuỗi sau đó tráo đổi phần đuôi của 2 cha mẹ cho nhau. Ví dụ: \underline{010|00} và 111|11 $\to$ \underline{010}|11 và 111|\underline{00}.

\textbf{2. Lai ghép 2 điểm:} Chọn 2 điểm cắt sau đó tráo đổi đoạn giữa 2 điểm cắt đó. \underline{01|00|10} và 11|11|01 tạo \underline{01}|11|\underline{10} và 11|\underline{00}|01.

\textbf{3. Lai ghép đồng nhất:} Xét từng vị trí bit. Bit của con được chọn từ cha hoặc mẹ với xác suất như nhau. Ví dụ: \underline{1}11\underline{01}0\underline{0}1 và 0\underline{00}01\underline{0}1\underline{0} $\to$ \underline{10001000} và 01110110.

\textbf{4. Chuỗi bit độ dài thay đổi:} Áp dụng khi giả thuyết có số lượng luật khác nhau hoặc cấu trúc động. Cần căn chỉnh các đoạn gen tương ứng (ví dụ: $A_1, A_2, C$) trước khi lai ghép để đảm bảo tính hợp lệ của con sinh ra. Ví dụ: \underline{A1|A2|C|A1|A2|C} và A1|A2|C|A1|A2|C $\to$ \underline{A1|A2|C|A1|A2}|C|A1|A2|\underline{C} và A1|A2|C.
}

% Ensemble Learning

\cheatbox{1. Tổng quan Ensemble Learning}{\textStrech}{
\footnotesize
\textbf{1. Định nghĩa:} Ensemble Learning là phương pháp kết hợp nhiều mô hình học máy để tạo ra một mô hình dự đoán tốt hơn.

\textbf{2. Các hướng tiếp cận chính:}
\begin{itemize}[leftmargin=*, nosep]
    \item \textbf{Averaging (Trung bình hóa):} Huấn luyện nhiều mô hình độc lập rồi lấy trung bình dự đoán (VD: Bagging).
    \item \textbf{Sequential (Tuần tự):} Huấn luyện chuỗi các mô hình, mô hình sau sửa lỗi cho mô hình trước (VD: Boosting).
    \item \textbf{Selection (Chọn lựa):} Chọn một mô hình tốt nhất để dự đoán dựa trên đầu vào cụ thể.
\end{itemize}
}

\cheatbox{2. Bagging (Bootstrap Aggregating)}{\textStrech}{
\footnotesize
\textbf{1. Bootstrap Data Sets:}
Từ tập dữ liệu gốc $X$ ($N$ mẫu), tạo ra tập $X_B$ bằng cách lấy mẫu ngẫu nhiên $N$ lần \textbf{có hoàn lại} sao cho một số điểm có thể lặp lại trong $X_B$, một số có thể vắng mặt.

\textbf{2. Quy trình Huấn luyện:} Tạo $M$ tập dữ liệu bootstrap khác nhau, sau đó huấn luyện mô hình trên mỗi tập dữ liệu để có $M$ mô hình cơ sở $y_m(x)$ cho dataset m.

\textbf{3. Kết hợp (Committee Prediction):}
\begin{itemize}[leftmargin=*, nosep]
    \item Lấy trung bình cộng các dự đoán (cho bài toán hồi quy):
    $$ y_{COM}(x) = \frac{1}{M} \sum_{m=1}^{M} y_m(x) $$
\end{itemize}

\textbf{4. Hiệu quả lý thuyết:} Nếu lỗi của các mô hình có trung bình bằng 0 và không tương quan, sai số bình phương trung bình của Bagging sẽ giảm đi $M$ lần so với mô hình đơn lẻ.
}

\cheatbox{3. Boosting (AdaBoost)}{\textStrech}{
\footnotesize
\textbf{1. Nguyên lý:} Huấn luyện tuần tự nhiều mô hình cơ sở. Mỗi mô hình tập trung weight cao hơn vào các điểm dữ liệu mà các mô hình trước đó phân loại sai. Kết quả cuối cùng là sự bầu chọn đa số có trọng số của tất cả mô hình cơ sở.

\textbf{2. Thuật toán AdaBoost (Chi tiết):}
\begin{itemize}[leftmargin=*, nosep]
    \item B1. Khởi tạo: Trọng số $w_n^{(1)} = 1/N$ cho mỗi điểm dữ liệu.
    \item B2. Lặp $m = 1 \dots M$:
    \begin{itemize}[leftmargin=*, nosep]
        \item Huấn luyện $y_m(x)$ cực tiểu hóa lỗi có trọng số $J_m = \sum_{n=1}^{N} w_n^{(m)} I(y_m(x_n) \ne t_n)$.
        \item Tính tỉ lệ lỗi: $\epsilon_m = \frac{\sum_{n=1}^{N} w_n^{(m)} I(y_m(x_n) \ne t_n)}{\sum_{n=1}^{N} w_n^{(m)}}$.
        \item Tính hệ số tín nhiệm: $\alpha_m = \ln \left( \frac{1-\epsilon_m}{\epsilon_m} \right)$.
        \item Cập nhật trọng số:
        $$ w_n^{(m+1)} = w_n^{(m)} \exp \{- \alpha_m I(y_m(x_n) \ne t_n) \} $$
    \end{itemize}
    \item B3. Dự đoán bằng mô hình cuối cùng:
    $$ Y_M(x) = \text{sign} \left( \sum_{m=1}^{M} \alpha_m y_m(x) \right) $$
\end{itemize}
}

\cheatbox{4. Các phương pháp khác}{\textStrech}{
\footnotesize
\begin{itemize}[leftmargin=*, nosep]
    \item Đưa ra kết quả chỉ khi quá nửa số mô hình cơ sở đồng ý.
    \item Xác suất ensemble chọn kết quả đúng là phân phối nhị thức: $\sum_{k= T/2+1}^{T} \binom{T}{k} p^k (1-p)^{T-k}$ với $p$ là xác suất mô hình cơ sở đúng và $T$ là số mô hình cơ sở.
    \item Nếu p > 0.5, xác suất này tăng dần về 1 khi T tăng đến $\infty$.
\end{itemize}
}

