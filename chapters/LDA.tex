\cheatbox{1. Giới thiệu về LDA}{\textStrech}{
\footnotesize
\textbf{1. Đầu vào:}
\begin{itemize}[leftmargin=*, nosep]
    \item X có kích thước $N \times D$, D rất lớn.
    \item $t_k$ là nhãn lớp, có C lớp khác nhau.
\end{itemize}

\textbf{2. Mục tiêu:}
\begin{itemize}[leftmargin=*, nosep]
    \item Giảm số chiều từ D xuống M và $M \leq C - 1$.
    \item Dữ liệu sau khi giảm có tính phân tách cao nhất.
\end{itemize}
}

\cheatbox{2. LDA qua API (Scikit-learn)}{\textStrech}{
\footnotesize
\begin{enumerate}
    \item from sklearn import datasets
    \item from sklearn.discriminant\_analysis import LinearDiscriminantAnalysis
    \item iris = datasets.load\_iris()
    \item X = iris.data
    \item y = iris.target
    \item target\_names = iris.target\_names
    \item lda = LinearDiscriminantAnalysis(n\_components=2)
    \item X\_r2 = lda.fit(X, y).transform(X)
\end{enumerate}
}

\cheatbox{3. Bài toán tối ưu}{\textStrech}{
\footnotesize
\textbf{1. Các thành phần:}
\begin{itemize}[leftmargin=*, nosep]
    \item Tâm của dữ liệu thuộc mỗi lớp: $m_1 = \frac{1}{N_1} \sum_{n \in C_1} x_n$, $m_2 = \frac{1}{N_2} \sum_{n \in C_2} x_n$
    \item Khoảng cách giữa 2 tâm khi chiếu lên vector $w$: $d_{12} = w^T S_B w$
    \item Between-class covariance matrix: $S_B = (m_2 - m_1)(m_1 - m_2)^T$
    \item Variance của lớp 1 sau khi chiếu lên $w$: $\sigma_1^2 = w^T S_{W1} w$
    \item Within-class covariance matrix: $S_{W1} = \sum_{n \in C_1} (x_n - m_1)(x_n - m_1)^T$
    \item Within-class covariance matrix của cả 2 lớp: $S_W = S_{W1} + S_{W2}$
    \item Within-class variance: $\sigma^2 = w^T S_W w$
\end{itemize}

\textbf{2. Hàm mục tiêu (Quan điểm của Fisher):}
\\ Tìm hướng $w^* = \arg\max_w J(w)$ để cực đại hóa tỷ số:
$$ \boxed{J(w) = \frac{w^T S_B w}{w^T S_W w}}$$

\textbf{3. Tìm nghiệm:}
\begin{itemize}[leftmargin=*, nosep]
    \item Đạo hàm $\nabla_w J = 0 <=> S_B w = \lambda S_W w$, $\lambda = \frac{w^T S_B w}{w^T S_W w}$
    \item => Phương trình eigenvector: $S_W^{-1} S_B w = \lambda w$
    \item Chúng ta chỉ cần hướng của $w$, nên dùng $S_B w = (m_2 - m_1)(m_2 - m_1)^T w$
    \item Hướng của vector $w$ là $S_W^{-1} (m_2 - m_1)$; Cần chuẩn hóa $w$ để có độ dài đơn vị.
\end{itemize}
}

\cheatbox{4. Trường hợp có C lớp}{\textStrech}{
\footnotesize
\textbf{1. Các thành phần:}
\begin{itemize}[leftmargin=*, nosep]
    \item Danh sách các vector cần tìm: $w_1, w_2, \dots, w_M$.
    \item Ma trận W (DxM) chứa các vector: $W = [w_1, w_2, \dots, w_M]$.
    \item Between-class var: $trace(W^T S_B W) = \sum_{m=1}^{M} w_m^T S_B w_m$.
    \item Within-class var: $trace(W^T S_W W) = \sum_{m=1}^{M} w_m^T S_W w_m$.
\end{itemize}

\textbf{2. Hàm mục tiêu và tìm nghiệm:}
\begin{itemize}[leftmargin=*, nosep]
    \item Hàm mục tiêu: $J(W) = \frac{trace(W^T S_B W)}{trace(W^T S_W W)}$.
    \item Mục tiêu: $W^* = \argmax_W J(W)$.
    \item Đạo hàm $\nabla_W J = 0 <=> S_B W = \lambda S_W W $.
    \item PT eigenvector: $S_W^{-1} S_B W = \lambda W$.
    \item Điều kiện để $S_W$ khả đảo: X phải có ít nhất D điểm độc lập tuyến tính và $N \geq D$.
    \item M chiều tìm được là M eigenvector của $S_W^{-1} S_B$ ứng với M eigenvalue lớn nhất.
\end{itemize}
}

\cheatbox{}{\textStrech}{
\footnotesize
\textbf{3. Tính toán các ma trận:}
\begin{itemize}[leftmargin=*, nosep]
    \item Tâm của tất cả các điểm dữ liệu: $m = \frac{1}{N} \sum_{n=1}^{N} x_n = \frac{1}{N} \sum_{k=1}^{C} N_k m_k$
    \item Between-class scatter matrix:
    $ S_B = \sum_{k=1}^{C} N_k (m_k - m)(m_k - m)^T $
    \item Within-class scatter matrix:
    $ S_W = \sum_{k=1}^{C} \sum_{n \in C_k} (x_n - m_k)(x_n - m_k)^T $
\end{itemize}
Tìm ma trận chiếu $W$ để tối đa hóa tỷ số định thức:

\textbf{4. Cách lựa chọn số chiều M:}
\begin{itemize}[leftmargin=*, nosep]
    \item Khi có C lớp, M lớn nhất là C-1 vì hạng $S_B$ tối đa là C-1.
    \item \textbf{Vì:} Ma trận $S_B$ là tổng có trọng số của C ma trận có hạng là 1.
    \item Bất kỳ $m_k$ nào cũng có thể tính qua C-1 vector $m_k$ còn lại.
    \item và $N_k = N - \sum_{i = 1}^{C|i\neq k} N_i$.
\end{itemize}

\textbf{5. Giải thuật:}
\begin{itemize}[leftmargin=*, nosep]
    \item Tính $S_B$ và $S_W$.
    \item Tính $A = S_W^{-1} S_B$.
    \item Sử dụng SVD để tìm ra M, M < C - 1, vector $w_m$ với m = 1, 2, \dots, M.
    \item Tính $Z = X - m^T$ (cần broadcast) với m là total mean = $\frac{1}{N} \sum_{n=1}^N x_n$.
    \item Dữ liệu sau khi thu giảm chiều: $\hat{X} = Z W$ với W chứa M vector $w_m$.
\end{itemize}
}